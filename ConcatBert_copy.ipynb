{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjGLMxBka9AE",
        "outputId": "577fdab3-1acf-4142-cc16-7730a0fd1642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Vc_YZQa30E",
        "outputId": "b78260a3-abe8-4ede-919e-0bd28c2d5e47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6uNYsIScy_X",
        "outputId": "c7f904d2-417f-4546-fdde-004bcfe9ac53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  2.0.0+cu118\n",
            "Torchvision Version:  0.15.1+cu118\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import copy\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel, BertModel\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "lldjD2aoc_L4"
      },
      "outputs": [],
      "source": [
        "num_epochs = 1\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "max_len = 431\n",
        "# Chargement du tokenizer et du modèle BERT\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_aGgdx1EawWZ"
      },
      "outputs": [],
      "source": [
        "train = pd.read_json(\"/content/drive/MyDrive/data/train.jsonl\",lines=True)\n",
        "test = pd.read_json(\"/content/drive/MyDrive/data/val.jsonl\",lines=True)\n",
        "del train['id']\n",
        "del test['id']\n",
        "\n",
        "\n",
        "def punctuation(df, column, new_column):\n",
        "  df[new_column]=df[column].apply(lambda x: \"\".join([char for char in x if char not in string.punctuation]))\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BVbIPocTawWa"
      },
      "outputs": [],
      "source": [
        "cleaned_train = punctuation(train, 'text', 'cleaned_text')\n",
        "del cleaned_train['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "yiyaG84lawWb"
      },
      "outputs": [],
      "source": [
        "\n",
        "cleaned_test = punctuation(test, 'text', 'cleaned_text')\n",
        "del cleaned_test['text']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, val_df = train_test_split(cleaned_train, test_size=1000, random_state=4)\n",
        "\n",
        "# Verify the sizes of the datasets\n",
        "print(f'Train dataset size: {len(train_df)}')\n",
        "print(f'Validation dataset size: {len(val_df)}')\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5yEtq7QgnIb",
        "outputId": "9d1ca9d4-062d-4e8e-f396-c1adcc3ea47c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 7500\n",
            "Validation dataset size: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "IEAGierEawWb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  'Caractérise un jeu de données pour PyTorch'\n",
        "  def __init__(self, transforms = None, root_dir = '/content/drive/MyDrive/data', mode = 'train', tokenizer = tokenizer):\n",
        "        'Initialisation'\n",
        "        if mode == 'train' : \n",
        "            self.df = train_df\n",
        "        elif mode == 'val' :\n",
        "            self.df = val_df\n",
        "        elif mode == 'test':\n",
        "            self.df = cleaned_test\n",
        "        self.labels = self.df.label\n",
        "        self.image_names = self.df.img\n",
        "        self.transforms = transforms\n",
        "        self.root_dir = root_dir\n",
        "        self.texts = [tokenizer(sentence, padding='max_length', max_length = max_len, truncation=True, return_tensors=\"pt\") for sentence in self.df['cleaned_text']]\n",
        "\n",
        "#        print(len(self.labels[self.labels == 0])/len(self.labels))\n",
        "      \n",
        "  def __len__(self):\n",
        "        \"Représente le nombre total d'exemples du jeu de données\"\n",
        "        return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "\n",
        "      image_path = f\"{self.root_dir}/{self.image_names.iloc[idx]}\"\n",
        "\n",
        "      img = Image.open(image_path, ).convert('RGB')\n",
        "\n",
        "      if self.transforms :\n",
        "            img = self.transforms(img)\n",
        "\n",
        "      \n",
        "\n",
        "      return self.texts[idx], img, self.labels.iloc[idx]\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtUORPgHdKB4",
        "outputId": "44c18541-2702-4695-e3d6-4314adc56a02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Datasets and Dataloaders...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "input_size = 224\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        \n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        \n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "\n",
        "    ])\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: MyDataset(transforms = data_transforms[x], mode = x) for x in ['train', 'val', 'test']}\n",
        "\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val', 'test']}\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFeL0Wyk4-9Z",
        "outputId": "faeaea79-9264-4399-cfc3-95dd1e2cebed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': <__main__.MyDataset at 0x7f5360f67d60>,\n",
              " 'val': <__main__.MyDataset at 0x7f5360f67340>,\n",
              " 'test': <__main__.MyDataset at 0x7f5360f678b0>}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "image_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "uAY-p7kXge6g"
      },
      "outputs": [],
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "    else :\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "def initialize_model( feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "    \n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    model_ft.fc = nn.Linear(num_ftrs, 1)\n",
        "    set_parameter_requires_grad(model_ft, feature_extract)\n",
        "    return model_ft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "fLI_SdYHp6rj"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from transformers import BertForSequenceClassification\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2, output_attentions = False, output_hidden_states = False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        bert_output = self.bert(input_ids = input_id, attention_mask = mask)\n",
        "        logits = bert_output.logits          # pour obtenir le tensor en sortie de bert\n",
        "        final_output = self.dropout(logits)  # pour réduire l'overfitt, ici p = 0.5\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0YWl7vBgm5u",
        "outputId": "1e042387-04cd-4f9b-cdb8-718366c193aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 191MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# définir le modèle Bert\n",
        "bert_model = BertClassifier()\n",
        "bert_model.load_state_dict(state_dict = torch.load('/content/drive/MyDrive/data/best_model_sigm(3).pth', map_location=torch.device('cpu')))\n",
        "\n",
        "# définir le modèle ResNet18\n",
        "resnet18_model = initialize_model( feature_extract= True )\n",
        "resnet18_model.load_state_dict(torch.load('/content/drive/MyDrive/data/best_model.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "vK11ayZikor4"
      },
      "outputs": [],
      "source": [
        "# définir une classe de modèle qui effectue la moyenne des sorties des deux modèles\n",
        "class ConcatBertModel(torch.nn.Module):\n",
        "    def __init__(self, bert_model, resnet18_model):\n",
        "        super(ConcatBertModel, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.resnet18_model = resnet18_model\n",
        "        self.fc = torch.nn.Linear(3, 1)\n",
        "        set_parameter_requires_grad(self.bert_model ,True)\n",
        "        set_parameter_requires_grad(self.resnet18_model ,True)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask, image):\n",
        "        bert_output = self.bert_model(input_id=input_ids, mask=attention_mask)\n",
        "\n",
        "        resnet_output = self.resnet18_model(image)\n",
        "      \n",
        "        concat_output = torch.cat((bert_output, resnet_output), dim=1)\n",
        "        output = self.fc(concat_output)\n",
        "\n",
        "        output = torch.sigmoid(output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CVLhfM8GpAU",
        "outputId": "ea051e5c-3635-4e0d-f209-4d80b798d70d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConcatBertModel(\n",
            "  (bert_model): BertClassifier(\n",
            "    (bert): BertForSequenceClassification(\n",
            "      (bert): BertModel(\n",
            "        (embeddings): BertEmbeddings(\n",
            "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder): BertEncoder(\n",
            "          (layer): ModuleList(\n",
            "            (0-11): 12 x BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                (intermediate_act_fn): GELUActivation()\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (pooler): BertPooler(\n",
            "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (activation): Tanh()\n",
            "        )\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (resnet18_model): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=3, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# initialiser une instance de ConcatBertModel\n",
        "concat_model = ConcatBertModel(bert_model, resnet18_model)\n",
        "concat_model = concat_model.to(device)\n",
        "print(concat_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "LIPrGS5b_3NE"
      },
      "outputs": [],
      "source": [
        "def train_concat_bert(model, dataloaders, criterion, optimizer, num_epochs):\n",
        "    since = time.time()\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.58\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            running_TP = 0.0\n",
        "            running_FP = 0.0\n",
        "            running_TN = 0.0\n",
        "            running_FN = 0.0\n",
        "            for input_ids, inputs, labels in tqdm(dataloaders[phase]):\n",
        "                TP=FP=TN=FN=0\n",
        "                inputs = inputs.to(device)\n",
        "                input_id = input_ids['input_ids'].squeeze(1).to(device)\n",
        "                mask = input_ids['attention_mask'].to(device)\n",
        "\n",
        "                labels = labels.to(device)\n",
        "              \n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # effectuer la prédiction et calculer la perte\n",
        "                    outputs = model(input_id, mask, inputs)\n",
        "                    loss = criterion(outputs, labels.float().unsqueeze(1).to(device))\n",
        "                \n",
        "                    if phase == 'train':\n",
        "                        # rétropropager et mettre à jour les poids\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                  \n",
        "                    preds = (outputs > 0.5).float()\n",
        "                    for i in range(len(preds)) :\n",
        "                      if preds[i]== 1 and labels[i]==1 : TP+=1\n",
        "                      elif preds[i]== 1 and labels[i]==0 : FP+=1\n",
        "                      elif preds[i]== 0 and labels[i]==0 : TN+=1\n",
        "                      elif preds[i]== 0 and labels[i]==1 : FN+=1\n",
        "                    \n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.float().unsqueeze(1))\n",
        "                running_TP += TP\n",
        "                running_FP += FP\n",
        "                running_TN += TN\n",
        "                running_FN += FN\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "            epoch_recall = running_TP / (running_TP + running_FN) # recall\n",
        "            epoch_specificity = running_TN / (running_TN + running_FP)\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f} Recall: {:.4f} Specificity: {:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_recall, epoch_specificity))\n",
        "            \n",
        "                # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), \"/content/drive/MyDrive/data/best_model_concat.pth\")\n",
        "\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return concat_bert, val_acc_history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test(model, criterion, device, batch_size):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    test_loader = dataloaders_dict['test']\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    running_TP = 0.0\n",
        "    running_FP = 0.0\n",
        "    running_TN = 0.0\n",
        "    running_FN = 0.0\n",
        "    with torch.no_grad():\n",
        "        for input_ids, inputs, labels in tqdm(test_loader):\n",
        "            TP=FP=TN=FN=0\n",
        "            inputs = inputs.to(device)\n",
        "            input_id = input_ids['input_ids'].squeeze(1).to(device)\n",
        "            mask = input_ids['attention_mask'].to(device)\n",
        "            labels = labels.to(device)\n",
        "         \n",
        "            outputs = model(input_id, mask, inputs)\n",
        "            loss = criterion(outputs, labels.float().unsqueeze(1).to(device))\n",
        "            preds = (outputs > 0.5).float()\n",
        "            for i in range(len(preds)) :\n",
        "              if preds[i]== 1 and labels[i]==1 : TP+=1\n",
        "              elif preds[i]== 1 and labels[i]==0 : FP+=1\n",
        "              elif preds[i]== 0 and labels[i]==0 : TN+=1\n",
        "              elif preds[i]== 0 and labels[i]==1 : FN+=1\n",
        "                    \n",
        "\n",
        "                # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.float().unsqueeze(1))\n",
        "            running_TP += TP\n",
        "            running_FP += FP\n",
        "            running_TN += TN\n",
        "            running_FN += FN\n",
        "    test_loss = running_loss / len(test_loader.dataset)\n",
        "    test_acc = running_corrects.double() / len(test_loader.dataset)\n",
        "    test_recall = running_TP / (running_TP + running_FN) # recall\n",
        "    test_specificity = running_TN / (running_TN + running_FP)\n",
        "    print('{} Loss: {:.4f} Acc: {:.4f} Recall: {:.4f} Specificity: {:.4f}'.format('test', test_loss, test_acc, test_recall, test_specificity))\n",
        "            \n"
      ],
      "metadata": {
        "id": "mZhYWXcsn-sW"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "mHoZRvrVAKye"
      },
      "outputs": [],
      "source": [
        "# définir la fonction de perte et l'optimiseur\n",
        "#criterion = torch.nn.BCEWithLogitsLoss(weight=torch.tensor([1, 1.787]))\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(concat_model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9IDkoodAdJk",
        "outputId": "8b2494a7-e173-490b-95f1-b6c49c63ec06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params to learn:\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ]
        }
      ],
      "source": [
        "print(\"Params to learn:\")\n",
        "params_to_update = []\n",
        "for name,param in concat_model.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "FHDm1QcDAlED",
        "outputId": "f10a3ba3-3378-43d0-d7c9-d38559e32247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/0\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|██████████| 938/938 [10:28<00:00,  1.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.7849 Acc: 0.5699 Recall: 0.0019 Specificity: 0.8896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [01:27<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.7705 Acc: 0.6510 Recall: 0.0000 Specificity: 1.0000\n",
            "Training complete in 12m 6s\n",
            "Best val Acc: 0.651000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-bb6424920f2b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconcat_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_concat_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-0bd3ed0809c9>\u001b[0m in \u001b[0;36mtrain_concat_bert\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_wts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcat_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'concat_bert' is not defined"
          ]
        }
      ],
      "source": [
        "concat_model, hist = train_concat_bert(concat_model, dataloaders_dict, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(concat_model, criterion, device, batch_size)"
      ],
      "metadata": {
        "id": "l5veBvo9vhFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TM_bMRIDfWR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "\n",
        "\n",
        "# Prédire les probabilités pour les données de test\n",
        "concat_model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    for input_ids, inputs, labels in dataloaders_dict['test']:\n",
        "        inputs = inputs.to(device)\n",
        "        input_id = input_ids['input_ids'].squeeze(1).to(device)\n",
        "        mask = input_ids['attention_mask'].to(device)\n",
        "\n",
        "        labels = labels\n",
        "              \n",
        "        outputs = concat_bert(input_id, mask, inputs)\n",
        "\n",
        "        y_pred.extend(torch.sigmoid(outputs).cpu().numpy())\n",
        "        y_true.extend(labels.numpy())\n",
        "\n",
        "\n",
        "# Calculer l'AUC-ROC\n",
        "auc_roc = roc_auc_score(y_true, y_pred)\n",
        "print(\"AUC-ROC : {:.4f}\".format(auc_roc))\n",
        "\n",
        "# Calculer la courbe ROC\n",
        "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Tracer la courbe ROC\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}