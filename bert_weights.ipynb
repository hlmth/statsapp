{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC8_C-AmTOH5"
      },
      "outputs": [],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvtNfNERHhNy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "oSkPrRdRllP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YiwSL8ZjkZo"
      },
      "outputs": [],
      "source": [
        "# getting the data\n",
        "train = pd.read_json(\"train.jsonl\",lines=True)\n",
        "dev = pd.read_json(\"dev.jsonl\",lines=True)\n",
        "del train['id']\n",
        "del train['img']\n",
        "del dev['id']\n",
        "del dev['img']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUixGfvpUVfw"
      },
      "outputs": [],
      "source": [
        "# functions to pre process text data : \n",
        "# 1. remove stopwords\n",
        "# 2. remove punctuation\n",
        "\n",
        "def stop_words(df, column, new_column):\n",
        "  df[new_column]=df[column].apply(lambda x: ' '.join([item for item in x.split() if item not in stopwords.words('english')]))\n",
        "  return df\n",
        "\n",
        "def punctuation(df, column, new_column):\n",
        "  df[new_column]=df[column].apply(lambda x: \"\".join([char for char in x if char not in string.punctuation]))\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLy5UqpLgzKb"
      },
      "outputs": [],
      "source": [
        "# pre processing the data\n",
        "cleaned_train = stop_words(train, 'text', 'cleaned_text')\n",
        "cleaned_train = punctuation(cleaned_train, 'cleaned_text', 'cleaned_text')\n",
        "cleaned_dev = stop_words(dev, 'text', 'cleaned_text')\n",
        "cleaned_dev = punctuation(cleaned_dev, 'cleaned_text', 'cleaned_text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAyzpEDIkGZ4"
      },
      "outputs": [],
      "source": [
        "text_train = cleaned_train.cleaned_text.values\n",
        "labels_train = cleaned_train.label.values\n",
        "text_dev = cleaned_dev.cleaned_text.values\n",
        "labels_dev = cleaned_dev.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a766HzgmlIaM"
      },
      "outputs": [],
      "source": [
        "train_input, val_input = text_train, text_dev\n",
        "train_label, val_label = labels_train, labels_dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQkhktRsluy3"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertModel, BertConfig, TFBertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_tM9tQ_l-YN"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06CVpbNYmU2D"
      },
      "outputs": [],
      "source": [
        "# Finding the maximum length\n",
        "max_len_train = 0\n",
        "\n",
        "for text in text_train :\n",
        "  max_len_train = max(max_len_train, len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKxpr_zCmTIZ"
      },
      "outputs": [],
      "source": [
        "# Finding the maximum length\n",
        "max_len_dev = 0\n",
        "\n",
        "for text in text_dev :\n",
        "  max_len_dev = max(max_len_dev, len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz-l4djypM7r"
      },
      "outputs": [],
      "source": [
        "# fonction d'encoding\n",
        "def mask_inputs_for_bert(text,max_len):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  i = 0\n",
        "  for t in text : \n",
        "    if (i<3):  # on affiche les 3 premiers textes\n",
        "      print(\"text :\", t)\n",
        "    encoded_dict =  tokenizer.encode_plus(t, add_special_tokens = True, max_length = max_len, pad_to_max_length = True, return_attention_mask = True)\n",
        "    if (i<3): # on affiche les 3 premiers textes tokenizÃ©s\n",
        "      print('dict :', encoded_dict['input_ids'])\n",
        "      print('attention masks :', encoded_dict['attention_mask'])\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "    i += 1\n",
        "  # convert totensor and return\n",
        "  input_ids = tf.convert_to_tensor(input_ids)\n",
        "  attention_masks = tf.convert_to_tensor(attention_masks)\n",
        "  return input_ids,attention_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItTyk9Gor9m_"
      },
      "outputs": [],
      "source": [
        "train_inp, train_mask = mask_inputs_for_bert(train_input,max_len_train)\n",
        "val_inp, val_mask = mask_inputs_for_bert(val_input,max_len_dev)\n",
        "train_label = tf.convert_to_tensor(train_label)\n",
        "val_label = tf.convert_to_tensor(val_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sS9Y1Si3tl37"
      },
      "outputs": [],
      "source": [
        "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2, return_dict=True, output_attentions = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StJ5bZDit5xC"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-6, epsilon = 1e-08, weight_decay=0.1)\n",
        "\n",
        "bert_model.compile(loss = loss, optimizer = optimizer, metrics = [metric])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuJqgjHtwx9I"
      },
      "outputs": [],
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  history = bert_model.fit([train_inp, train_mask],\\\n",
        "                         train_label,\\\n",
        "                         batch_size = 16,\\\n",
        "                         epochs = 4,\\\n",
        "                         validation_data = ([val_inp, val_mask], val_label))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_weights = bert_model.get_weights() # returns a list consisting of NumPy arrays"
      ],
      "metadata": {
        "id": "Hy-l1Fpt1Y34"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}