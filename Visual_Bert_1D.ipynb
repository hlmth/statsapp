{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0pCxo9EjB7b",
        "outputId": "0dc7df87-f7ed-4510-db18-6240a7797d01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\arman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\arman\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in c:\\users\\arman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\arman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\arman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\arman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\arman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\arman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\arman\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: colorama in c:\\users\\arman\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\arman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\arman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zw520vWukDFq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import VisualBertForPreTraining, BertTokenizer, VisualBertModel, VisualBertForVisualReasoning\n",
        "from transformers import VisualBertConfig\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o_eGA-pE-ghv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8iZmOtrtaGw"
      },
      "source": [
        "Définition des classes dont on a besoin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FOp6T7jckDFr"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.visual_bert = VisualBertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.linear = nn.Linear(config.hidden_size, 1)\n",
        "        self.attentions = None  # Add an `attentions` attribute to the class\n",
        "        self.pooler_output  = None\n",
        "        self.last_hidden_state = None\n",
        "\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        visual_embeds=None,\n",
        "        visual_attention_mask=None,\n",
        "        visual_token_type_ids=None,\n",
        "    ):\n",
        "        outputs  = self.visual_bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            visual_embeds=visual_embeds,\n",
        "            visual_attention_mask=visual_attention_mask,\n",
        "            visual_token_type_ids=visual_token_type_ids,\n",
        "            output_attentions=True\n",
        "        )\n",
        "        otp=outputs.pooler_output \n",
        "        pooled_output = self.dropout(otp)\n",
        "        linear_output = self.linear(pooled_output)\n",
        "        output=torch.sigmoid(linear_output)\n",
        "        attentions = outputs.attentions # extract attention scores from the model outputs\n",
        "\n",
        "        return output, attentions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gTnWeyMYkDFs"
      },
      "outputs": [],
      "source": [
        "# Define the training dataset\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.indices = list(range(len(data)))  # set indices attribute\n",
        "        print(self.data.keys())  \n",
        "        print(f\"Number of indices: {len(self.indices)}\")\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.indices[index]  # get the actual index from self.indices\n",
        "        text = self.data['text'][index]\n",
        "        label = self.data['label'][index]\n",
        "        embedded = self.data['embedded'][index]\n",
        "        \n",
        "        return text, label, embedded\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-MNUFTltg_x"
      },
      "source": [
        "Chargement du modèle pré entrainé et customisation du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nzceoJ_okDFt"
      },
      "outputs": [],
      "source": [
        "# Define the pre-trained Visual-Bert model\n",
        "\n",
        "config = VisualBertConfig.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre')\n",
        "model = Model(config)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gisg0ePpsbUY"
      },
      "outputs": [],
      "source": [
        "for layer in model.children():\n",
        "    if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WjOChudAkDFu"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cOyIBXStlzi"
      },
      "source": [
        "Définition des fonctions d'entrainement et de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-3sDnaUxkDFu"
      },
      "outputs": [],
      "source": [
        "# Define the training loop\n",
        "def train(model, tokenizer, train_dataset, valid_dataset, optimizer, criterion, device, batch_size, epochs, patience):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    best_valid_loss = float('inf')\n",
        "    patience_count = 0\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        running_accuracy = 0.0\n",
        "        running_TP = 0.0\n",
        "        running_FP = 0.0\n",
        "        running_TN = 0.0\n",
        "        running_FN = 0.0\n",
        "        for batch in train_loader:\n",
        "            text, label, embedded = batch\n",
        "            optimizer.zero_grad()\n",
        "            text_encoded = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "            text_encoded = {k: v.to(device) for k, v in text_encoded.items()}\n",
        "            label = label.float().unsqueeze(1).to(device)\n",
        "            inputs_ids=text_encoded['input_ids'].to(device)\n",
        "            visual_embeds = embedded.to(device)\n",
        "            attention_mask = text_encoded['attention_mask'].to(device)\n",
        "            outputs, attentions = model(input_ids=inputs_ids, attention_mask=attention_mask, visual_embeds=visual_embeds)\n",
        "            print(outputs)\n",
        "            loss = criterion(outputs, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            pred_labels = torch.round(outputs) # round the probabilities to obtain predicted labels\n",
        "            correct_preds = (pred_labels == label).sum().item() # count the number of correct predictions\n",
        "            accuracy = correct_preds / batch_size # calculate accuracy\n",
        "            running_loss += loss.item()\n",
        "            running_accuracy += accuracy\n",
        "                \n",
        "            # Calculate TP, FP, TN, FN\n",
        "            TP = ((pred_labels == 1) & (label == 1)).sum().item()\n",
        "            FP = ((pred_labels == 1) & (label == 0)).sum().item()\n",
        "            TN = ((pred_labels == 0) & (label == 0)).sum().item()\n",
        "            FN = ((pred_labels == 0) & (label == 1)).sum().item()\n",
        "            running_TP += TP\n",
        "            running_FP += FP\n",
        "            running_TN += TN\n",
        "            running_FN += FN\n",
        "       \n",
        "          \n",
        "            \n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_accuracy = running_accuracy / len(train_loader)\n",
        "        epoch_recall = running_TP / (running_TP + running_FN) # recall\n",
        "        epoch_specificity = running_TN / (running_TN + running_FP) # specificity\n",
        "        valid_loss = 0.0\n",
        "        valid_accuracy = 0.0\n",
        "\n",
        "        for batch in valid_loader:\n",
        "            text, label, embedded = batch\n",
        "            text_encoded = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "            text_encoded = {k: v.to(device) for k, v in text_encoded.items()}\n",
        "            label = label.float().unsqueeze(1).to(device)\n",
        "            inputs_ids = text_encoded['input_ids'].to(device)\n",
        "            visual_embeds = embedded.to(device)\n",
        "            attention_mask = text_encoded['attention_mask'].to(device)\n",
        "            outputs, attentions = model(input_ids=inputs_ids, attention_mask=attention_mask, visual_embeds=visual_embeds)\n",
        "            loss = criterion(outputs, label)\n",
        "            pred_labels = torch.round(outputs) # round the probabilities to obtain predicted labels\n",
        "            correct_preds = (pred_labels == label).sum().item() # count the number of correct predictions\n",
        "            accuracy = correct_preds / batch_size # calculate accuracy\n",
        "            valid_loss += loss.item()\n",
        "            valid_accuracy += accuracy\n",
        "          \n",
        "        valid_loss /= len(valid_loader)\n",
        "        valid_accuracy /= len(valid_loader)\n",
        "\n",
        "        print('Epoch [%d] - loss: %.4f - accuracy: %.4f - recall: %.4f - specificity: %.4f - val_loss: %.4f ' % (epoch+1, epoch_loss, epoch_accuracy, epoch_recall, epoch_specificity,  valid_loss))\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            patience_count = 0\n",
        "        else:\n",
        "            patience_count += 1\n",
        "            if patience_count >= patience:\n",
        "                print(\"Validation loss did not improve for %d epochs. Training stopped early.\" % patience)\n",
        "                break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "h4f0Mmtj2Ep-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test(model, tokenizer, test_dataset, criterion, device, batch_size):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "    running_TP = 0.0\n",
        "    running_FP = 0.0\n",
        "    running_TN = 0.0\n",
        "    running_FN = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "              text, label, embedded = batch\n",
        "              text_encoded = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "              text_encoded = {k: v.to(device) for k, v in text_encoded.items()}\n",
        "              label = label.float().unsqueeze(1).to(device)\n",
        "              inputs_ids=text_encoded['input_ids'].to(device)\n",
        "              visual_embeds = embedded.to(device)\n",
        "              attention_mask = text_encoded['attention_mask'].to(device)\n",
        "              outputs , attentions = model(input_ids=inputs_ids, attention_mask=attention_mask, visual_embeds=visual_embeds)\n",
        "              loss = criterion(outputs, label)\n",
        "              pred_labels = torch.round(outputs) # round the probabilities to obtain predicted labels\n",
        "              correct_preds = (pred_labels == label).sum().item() # count the number of correct predictions\n",
        "              accuracy = correct_preds / batch_size # calculate accuracy\n",
        "              running_loss += loss.item()\n",
        "              running_accuracy += accuracy\n",
        "\n",
        "                # Calculate TP, FP, TN, FN\n",
        "              TP = ((pred_labels == 1) & (label == 1)).sum().item()\n",
        "              FP = ((pred_labels == 1) & (label == 0)).sum().item()\n",
        "              TN = ((pred_labels == 0) & (label == 0)).sum().item()\n",
        "              FN = ((pred_labels == 0) & (label == 1)).sum().item()\n",
        "              running_TP += TP\n",
        "              running_FP += FP\n",
        "              running_TN += TN\n",
        "              running_FN += FN\n",
        "                \n",
        "         \n",
        "    test_loss = running_loss / len(test_loader)\n",
        "    test_accuracy = running_accuracy / len(test_loader)\n",
        "    test_recall = running_TP/ (running_TP + running_FN) # recall\n",
        "    test_specificity = running_TN / (running_TN + running_FP) # specificity\n",
        "    print('Test loss: %.4f - Test accuracy: %.4f- recall: %.4f - specificity: %.4f' % (test_loss, test_accuracy,test_recall,test_specificity))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BVCrGwW_kDFv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.0001)\n",
        "criterion = nn.BCELoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc2dXuXGAivX",
        "outputId": "c8aefab8-c7ff-449e-8bcc-08e31b7777d6"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8EqAgkFkDFv"
      },
      "outputs": [],
      "source": [
        "df_test=pd.read_json(\"C:/Users/arman/OneDrive/Bureau/data/dev.jsonl\",lines=True)\n",
        "\n",
        "m = []\n",
        "\n",
        "with open(r\"C:/Users/arman/OneDrive/Bureau/data/file_val.pkl\",\"rb\") as g:\n",
        "    m = pickle.load(g)\n",
        "g.close()\n",
        "df_test.loc[:, 'embedded'] = m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2VKr5Ja2cD2"
      },
      "outputs": [],
      "source": [
        "df_train=pd.read_json(\"C:/Users/arman/OneDrive/Bureau/data/train.jsonl\",lines=True)\n",
        "\n",
        "l = []\n",
        "\n",
        "with open(r\"C:/Users/arman/OneDrive/Bureau/data/file.pkl\",\"rb\") as f:\n",
        "    l = pickle.load(f)\n",
        "f.close()\n",
        "df_train.loc[:, 'embedded'] = l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY67Z40BptdM",
        "outputId": "7e51facd-0b0f-4c11-d6ce-4ccf294c44f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8464\n"
          ]
        }
      ],
      "source": [
        "mask_train = pd.to_numeric(df_train['embedded'], errors='coerce').isna()\n",
        "df_train = df_train[mask_train]\n",
        "print(len(df_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR-_vNsObJFn",
        "outputId": "555ee016-9c6f-4b55-b8a3-a21f1710d5ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 7964\n",
            "Validation dataset size: 500\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, valid_df = train_test_split(df_train, test_size=500, random_state=42)\n",
        "\n",
        "# Verify the sizes of the datasets\n",
        "print(f'Train dataset size: {len(train_df)}')\n",
        "print(f'Validation dataset size: {len(valid_df)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSdGjfjIgeJa"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.reset_index(drop=True)\n",
        "valid_df = valid_df.reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alHqGqRLkDFw",
        "outputId": "39a67b2f-ce21-4ec5-9693-cdd143eaeda8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['id', 'img', 'label', 'text', 'embedded'], dtype='object')\n",
            "Number of indices: 7964\n",
            "7964\n",
            "Index(['id', 'img', 'label', 'text', 'embedded'], dtype='object')\n",
            "Number of indices: 500\n",
            "500\n"
          ]
        }
      ],
      "source": [
        "train_dataset = MyDataset(train_df)\n",
        "print(len(train_dataset))\n",
        "valid_dataset = MyDataset(valid_df)\n",
        "print(len(valid_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "kEVoXxzqkDFx",
        "outputId": "9f4862db-6cdd-4947-b3bd-2264e37bb778"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 19660800 bytes.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(model, tokenizer, train_dataset, valid_dataset, optimizer, criterion, device, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
            "Cell \u001b[1;32mIn[9], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, tokenizer, train_dataset, valid_dataset, optimizer, criterion, device, batch_size, epochs, patience)\u001b[0m\n\u001b[0;32m     23\u001b[0m visual_embeds \u001b[39m=\u001b[39m embedded\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     24\u001b[0m attention_mask \u001b[39m=\u001b[39m text_encoded[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 25\u001b[0m outputs, attentions \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49minputs_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, visual_embeds\u001b[39m=\u001b[39;49mvisual_embeds)\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(outputs)\n\u001b[0;32m     27\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, label)\n",
            "File \u001b[1;32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[1;32mIn[4], line 22\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask, visual_token_type_ids)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m     14\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     15\u001b[0m     input_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     visual_token_type_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m ):\n\u001b[1;32m---> 22\u001b[0m     outputs  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual_bert(\n\u001b[0;32m     23\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m     24\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m     25\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m     26\u001b[0m         visual_embeds\u001b[39m=\u001b[39;49mvisual_embeds,\n\u001b[0;32m     27\u001b[0m         visual_attention_mask\u001b[39m=\u001b[39;49mvisual_attention_mask,\n\u001b[0;32m     28\u001b[0m         visual_token_type_ids\u001b[39m=\u001b[39;49mvisual_token_type_ids,\n\u001b[0;32m     29\u001b[0m         output_attentions\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m     )\n\u001b[0;32m     31\u001b[0m     otp\u001b[39m=\u001b[39moutputs\u001b[39m.\u001b[39mpooler_output \n\u001b[0;32m     32\u001b[0m     pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(otp)\n",
            "File \u001b[1;32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\visual_bert\\modeling_visual_bert.py:813\u001b[0m, in \u001b[0;36mVisualBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, visual_embeds, visual_attention_mask, visual_token_type_ids, image_text_alignment, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    811\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 813\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[0;32m    814\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    815\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    816\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m    817\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    818\u001b[0m     visual_embeds\u001b[39m=\u001b[39;49mvisual_embeds,\n\u001b[0;32m    819\u001b[0m     visual_token_type_ids\u001b[39m=\u001b[39;49mvisual_token_type_ids,\n\u001b[0;32m    820\u001b[0m     image_text_alignment\u001b[39m=\u001b[39;49mimage_text_alignment,\n\u001b[0;32m    821\u001b[0m )\n\u001b[0;32m    823\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbypass_transformer \u001b[39mand\u001b[39;00m visual_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    824\u001b[0m     text_length \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\visual_bert\\modeling_visual_bert.py:188\u001b[0m, in \u001b[0;36mVisualBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, visual_embeds, visual_token_type_ids, image_text_alignment)\u001b[0m\n\u001b[0;32m    183\u001b[0m         visual_position_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\n\u001b[0;32m    184\u001b[0m             \u001b[39m*\u001b[39mvisual_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mvisual_embeds\u001b[39m.\u001b[39mdevice\n\u001b[0;32m    185\u001b[0m         )\n\u001b[0;32m    186\u001b[0m         visual_position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisual_position_embeddings(visual_position_ids)\n\u001b[1;32m--> 188\u001b[0m     visual_embeddings \u001b[39m=\u001b[39m visual_embeds \u001b[39m+\u001b[39;49m visual_position_embeddings \u001b[39m+\u001b[39;49m visual_token_type_embeddings\n\u001b[0;32m    190\u001b[0m     embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((embeddings, visual_embeddings), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    192\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 19660800 bytes."
          ]
        }
      ],
      "source": [
        "train(model, tokenizer, train_dataset, valid_dataset, optimizer, criterion, device, batch_size=32, epochs=10, patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucNwHXW8fzk0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcnrYQ62YVlx"
      },
      "outputs": [],
      "source": [
        "mask_test = pd.to_numeric(df_test['embedded'], errors='coerce').isna()\n",
        "df_test = df_test[mask_test]\n",
        "print(len(df_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4cZ2S58Z0TJ"
      },
      "outputs": [],
      "source": [
        "df_test = df_test.reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GASWgEJvYBa9"
      },
      "outputs": [],
      "source": [
        "test_dataset = MyDataset(df_test)\n",
        "print(len(test_dataset))  # should print the length of your train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUI81o0XX6Ff"
      },
      "outputs": [],
      "source": [
        "test(model, tokenizer, test_dataset, criterion, device, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mfu_By2RcQpM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last=False)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "   \n",
        "    for batch in test_loader:\n",
        "          text, label, embedded = batch\n",
        "          text_encoded = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "          text_encoded = {k: v.to(device) for k, v in text_encoded.items()}\n",
        "          label = label.float().unsqueeze(1).to(device)\n",
        "          inputs_ids=text_encoded['input_ids'].to(device)\n",
        "          visual_embeds = embedded.to(device)\n",
        "          attention_mask = text_encoded['attention_mask'].to(device)\n",
        "          outputs, attentions = model(input_ids=inputs_ids, attention_mask=attention_mask, visual_embeds=visual_embeds)\n",
        "          outputs_cpu=outputs[0:].cpu()\n",
        "          label_cpu=label[0:].cpu()\n",
        "          #print(outputs[0:])\n",
        "          y_pred.extend(outputs_cpu.numpy())\n",
        "          y_true.extend(label_cpu.numpy())\n",
        "          pred_labels = torch.round(outputs[0]) # round the probabilities to obtain predicted labels\n",
        "          correct_preds = (pred_labels == label).sum().item() # count the number of correct predictions\n",
        "\n",
        "\n",
        "print(len(y_pred))\n",
        "print(len(y_true))\n",
        "# Calculer l'AUC-ROC\n",
        "\n",
        "auc_roc = roc_auc_score(y_true, y_pred)\n",
        "print(\"AUC-ROC : {:.4f}\".format(auc_roc))\n",
        "\n",
        "# Calculer la courbe ROC\n",
        "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Tracer la courbe ROC\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb_93mOchybd"
      },
      "outputs": [],
      "source": [
        "  model.to(device)\n",
        "  with torch.no_grad():\n",
        "  \n",
        "    for batch in test_loader:\n",
        "    \n",
        "              text, label, embedded = batch\n",
        "              text_encoded = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "              text_encoded = {k: v.to(device) for k, v in text_encoded.items()}\n",
        "              label = label.float().unsqueeze(1).to(device)\n",
        "              inputs_ids=text_encoded['input_ids'].to(device)\n",
        "              visual_embeds = embedded.to(device)\n",
        "              attention_mask = text_encoded['attention_mask'].to(device)\n",
        "              outputs, attentions = model(input_ids=inputs_ids, attention_mask=attention_mask, visual_embeds=visual_embeds)\n",
        "     \n",
        "\n",
        "              # Extraire les scores d'attention par couche\n",
        "              #attentions = outputs.attentions\n",
        "              for layer, attention in enumerate(attentions):\n",
        "                print(f\"Layer {layer+1} attention shape: {attention.shape}\")\n",
        "              # Extraire les scores d'attention par tête et par couche\n",
        "              #multi_head_attention = outputs.multi_head_attention_outputs\n",
        "              #for layer, attention_layer in enumerate(multi_head_attention):\n",
        "                 #print(f\"Layer {layer+1} attention shape: {[attention.shape for attention in attention_layer]}\")\n",
        "              \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGNk2pUulf0e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=False)\n",
        "for batch in test_loader  : \n",
        "\n",
        "  # Load the image\n",
        "\n",
        "\n",
        "    text, label, embedded = batch\n",
        "    text_encoded = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "    text_encoded = {k: v.to(device) for k, v in text_encoded.items()}\n",
        "    label = label.float().unsqueeze(1).to(device)\n",
        "    inputs_ids=text_encoded['input_ids'].to(device)\n",
        "    visual_embeds = embedded.to(device)\n",
        "    attention_mask = text_encoded['attention_mask'].to(device)\n",
        "    outputs, attentions = model(input_ids=inputs_ids, attention_mask=attention_mask, visual_embeds=visual_embeds)\n",
        "    break\n",
        "  #Resize the image to match the input size of the model\n",
        "  #img = cv2.resize(img, (224, 224))\n",
        "  #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "img_path = df_test['img'][1]\n",
        "path='/content/drive/MyDrive/data/'+img_path\n",
        "#img = cv2.imread('/content/drive/MyDrive/data/'+img_path)\n",
        "img= cv2.imread('/content/drive/MyDrive/data/img/01235.png')\n",
        "# Get the attention scores for the image\n",
        "attention_scores = attentions[0][0][0].detach().cpu().numpy()\n",
        "\n",
        "attention_scores = cv2.resize(attention_scores, (img.shape[1], img.shape[0]))\n",
        "attention_scores = (attention_scores - attention_scores.min()) / (attention_scores.max() - attention_scores.min())\n",
        "heatmap = cv2.applyColorMap(np.uint8(255*attention_scores), cv2.COLORMAP_JET)\n",
        "\n",
        "result = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "plt.imshow(result)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "ee4c60ed9f2d143d855c3de7cdf8e81ef9399a07140d5084c19be7d5d534dff2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
